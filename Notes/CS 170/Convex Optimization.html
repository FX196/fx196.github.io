<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <title>Convex Optimization</title>
    
    <!-- Bootstrap core CSS -->
    <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Custom fonts for this template -->
    <link href="../../assets/vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
          type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
          rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Quattrocento" rel="stylesheet">
    
    <!-- MathJax Import -->
    <script async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
            type="text/javascript">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    
    <script language="JavaScript" src="../../assets/js/main.js" type="text/javascript"></script>
    
    <script language="JavaScript" src="../../assets/js/cs170.js" type="text/javascript"></script>
    
    <!-- Custom styles for this template -->
    <link href="../../assets/css/main.css" rel="stylesheet">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137822561-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-137822561-1');
    </script>


</head>
<body>

<div class="linkshare-image">
    <img src="../../assets/img/meta-logo.jpeg" alt="unable to display image">
</div>
<!-- Header and Nav Bar -->
<iframe id="header-frame" scrolling="no" src="../../assets/frames/header.html"></iframe>


<!-- Main Content -->
<h2 align="center" class="notes-title" style="padding-bottom: 20px">Convex Optimization</h2>

<div class="container note-section">
    <ul>
        <li class="note-block">
            <h3>Problem Setup</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item" style="list-style: none">
                    \(
                    \begin{align*}
                    \text{Time} &\quad& \text{choice 1} &\quad& \text{choice 2} &\quad& \cdots &\quad& \text{choice n}\\
                    1 && x^{(1)}_1, l^{(1)}_1 && x^{(1)}_2, l^{(1)}_2 && \cdots && x^{(1)}_n, l^{(d)}_n\\
                    2 && x^{(2)}_1, l^{(2)}_1 && x^{(2)}_2, l^{(2)}_2 && \cdots && x^{(2)}_n, l^{(d)}_n\\
                    \vdots && \ddots \quad && \ddots \quad && \ddots && \vdots \qquad \\
                    t && x^{(t)}_1, l^{(t)}_1 && x^{(t)}_2, l^{(t)}_2 && \cdots && x^{(t)}_n, l^{(d)}_n\\
                    \vdots && \ddots \quad && \ddots \quad && \ddots && \vdots \qquad \\
                    T && x^{(T)}_1, l^{(T)}_1 && x^{(T)}_2, l^{(T)}_2 && \cdots && x^{(T)}_n, l^{(d)}_n\\
                    
                    \end{align*}
                    \)<br>
                    Conditions:
                    \(
                    \forall t, x^{(t)}_i \geq 0; \\
                    \sum \limits_{i=1}^n x^{(t)}_i = 1;
                    \)
                </li>
                <li class="note-item">
                    At step \(t\) the algorithm has loss \(\sum \limits_{i=1}^n x^{(t)}_i \cdot l^{(t)}_i\)
                </li>
                <li class="note-item">
                    \(l^{(t)}_i\) is unknown ahead of time
                </li>
                <li class="note-item">
                    <i>Regret</i>: actual performance minus best fixed strategy w/ hindsight <br>
                    \(R_T = [\sum \limits_{t=1}^T \sum \limits_{i=1}^n x^{(i)}_t l^{(i)}_t] - [\min \limits_{i=1\cdots
                    n} \sum \limits_{t=1}^T l^{(t)}_i]\) <br>
                    Comparing against someone who knew everything but is only able to make a fixed choice
                </li>
                <li class="note-item">
                    <i>Offline optimum</i>: regret = 0
                </li>
                <li class="note-item">
                    The actions performed are <i>online</i> since we do not have access to loss beforehand
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Multiplicative Weights </h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Stores weights \(w^{(t)}_i\) initialized to 1
                </li>
                <li class="note-item">
                    parameterized by \(0<\varepsilon<\frac{1}{2}\)
                </li>
                <li class="note-item">
                    update: \(w^{(t+1)}_i = (1-\varepsilon)^{l^{(t)}_i} \) <br>
                    update penalizes weights that generate loss
                </li>
                <li class="note-item">
                    At time \(t\) play \(x^{(T)}_i = \frac{w^{(t)}_i}{\sum w^{(t)}_i}\)
                </li>
                <li class="note-item">
                    Guarantees \(R_T \leq O(\sqrt{T\log n})\) <br>
                    And \(\frac{R_T}{T} \leq O(\sqrt{\frac{\log n}{T}})\), \(\frac{R_T}{T} \rightarrow 0\) as \(T
                    \rightarrow \infty\)
                </li>
                <li class="note-item">
                    Regret bound:<br>
                    \(\begin{align*}
                    \sum \limits_{t=1}^T \sum \limits_{i=1}^n x^{(i)}_t l^{(i)}_t &\quad \leq &\min \limits_{x^*}\sum
                    \limits_{t=1}^T \sum \limits_{i=1}^n x^*_t l^{(i)}_t &\quad+ \varepsilon T + \frac{\ln
                    n}{\varepsilon} \\
                    \text{loss of algorithm} && \text{offline optimum} & \qquad \text{regret}
                    \end{align*}\) <br>
                    Where \(x^*\) is a fixed strategy
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Multiplicative Weights Analysis</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Assume \(l \in [0,1], \varepsilon \in [0, \frac{1}{2}\)
                </li>
                <li class="note-item">
                    Definitions: <br>
                    \(L_t = \sum \limits_{i=1}^n x^{(i)}_t l^{(i)}_t\); loss at time \(t\) <br>
                    \(L^* = \min \limits_{i} \sum \limits_{t=1}^T l^{(t)}_i\); offline optimum <br>
                    \(\varepsilon T + \frac{\ln n}{\varepsilon}\); regret <br>
                    \(W_t = \sum \limits_{i=1}^n w^{(t)}_i\); total weight at time \(t\)
                </li>
                <li class="note-item">
                    Lemma 1:<br>
                    \(W_{t+1} \geq (1-\varepsilon)^{L^*}\) <br>
                    Proof: <br>
                    \(w^{(t+1)}_i = (1-\varepsilon)^{l^{(1)}_i + l^{(2)}_i + l^{(3)}_i \cdots l^{(T)}_i}\) <br>
                    \(W_t \geq w^{(t)}_i, \forall i\) <br>
                    \(L^* = \sum l^{(1)}_i \cdots l^{(T)}_i\) for some \(i\)
                
                </li>
                <li class="note-item">
                    Lemma 2:<br>
                    \(W_{t+1} \leq n \cdot \prod \limits_{t=1}^T (1-\varepsilon L_t)\) <br>
                    Proof: <br>
                    \(W_1 = n\) since all weights are initialized as 1 <br>
                    Claim that \(W_{t+1} \leq W_t \cdot (1-\varepsilon L_t)\) <br>
                    \(\begin{align*}
                    W_{t+1} &= \sum w_i^{(t+1)} = \sum \limits_{i=1}^{n} w_{i}^{(t)}
                    \cdot(1-\varepsilon)^{\ell_{i}^{(t)}} \\
                    &= W_{t} \sum_{i=1}^{n} x_{i}^{(t)} \cdot(1-\varepsilon)^{\ell_{i}^{(t)}} \\
                    &\leq W_{t} \sum_{i=1}^{n} x_{i}^{(t)}\left(1-\varepsilon \cdot l_{i}^{( t )}\right) \\
                    &=W_{t} \cdot\left(\sum_{i=1}^{n} x_{i}^{(t)}-\sum_{i=1}^{n} \varepsilon x_{i}^{t}
                    l_{i}^{(t)}\right) \\
                    &=W_t \cdot (1-\varepsilon L_t)
                    \end{align*}\)
                </li>
                <li class="note-item">
                    From lemmas 1 and 2: <br>
                    \(\begin{align*}
                    (1-\varepsilon)^{L^*} &\leq n \prod\limits_{t=1}^{T}\left(1-\varepsilon L_{t}\right) \\
                    \ln (1-\varepsilon)^{L^*} &\leq \ln \left(n \prod_{t=1}^{T}\left(1-\varepsilon L_{t}\right)\right)
                    \\
                    L^{*} \ln (1-\varepsilon) &\leq \ln n+\sum_{t=1}^{T} \ln \left(1-\varepsilon L_{t}\right)
                    \end{align*}\)
                    <!--Finish Proof TODO-->
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Multiplicative Weights: More General Losses</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    TODO
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Follow the Leader</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    A more general case / parent of MW
                </li>
                <li class="note-item">
                    Setup: <br>
                    \(\begin{align*}
                    \text{Time} &\quad& \text{Algorithm} &\quad& \text{Loss} &\quad& \text{Loss of Algorithm} \\
                    t=1 && x^{(1)} \in K && f_1:K \rightarrow R && f_1(x^{(1)}) \\
                    t=2 && x^{(2)} \in K && f_2:K \rightarrow R && f_2(x^{(2)}) \\
                    \vdots && \vdots && \vdots && \vdots \\
                    \end{align*}\)
                </li>
                <li class="note-item">
                    at time t \( x^{(t)} = \arg\min\limits_{x \in K} f_{1}(x)+\ldots f_{t-1}(x)\)
                </li>
                <li class="note-item">
                    does badly in an interesting way <br>
                    keeps changing its mind and choosing sub-optimal choices
                </li>
                <li class="note-item">
                    Regret can be bounded in with respect to how much the algorithm changes its mind
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>FTL Analysis</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Theorem: \(R_T \leq \sum\limits_{t=1}^{T} f_{t}\left(x^{(t)}\right)-f_{t}\left(x^{(t+1)}\right)\)
                    <br>
                    i.e. bounded by how much the loss function changes
                </li>
                <li class="note-item">
                    Proof: Induction on T <br>
                    \(\sum\limits_{t=1}^{T} f_{t}\left(x^{(t+1)}\right) \leq \min _{x \in \Delta} \sum\limits_{t=1}^{T}
                    f_{t}(x)\)
                    <br>
                    base case: t=1, by definition <br>
                    assume true for t, prove for t+1 <br>
                    \(\begin{align*}
                    \sum_{t=1}^{T+1} f_{t}\left(x^{(t+1)}\right) &=f_{T+1}\left(x^{(T+2)}\right)+\sum_{t=1}^{T+1}
                    f_{t}\left(x^{(t+1)}\right) \\
                    & \leq f_{T+1}\left(x^{(T+2)}\right)+\sum_{t=1}^{T} f_{t}\left(x^{(T+2)}\right) \\
                    & = \sum_{t=1}^{T+1} f_{t}\left(x^{(T+2)}\right)
                    \end{align*}\)
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Follow the Regularized Leader</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    define a function \(R: K \rightarrow R\)
                </li>
                <li class="note-item">
                    Algorithm: \(x^{(t)}=\arg\min\limits_{x \in K} f_{1}(x)+ \cdots +f_{t-1}(x)+R(x)\)
                </li>
                <li class="note-item">
                    Pick an \(R\) that has an unique minimum and is very smooth
                </li>
                <li class="note-item">
                    New regret bound: <br>
                    \(\sum_{t=1}^{T} f_{t}\left(x^{(t)}\right)-\sum_{t=1}^{T} f_{t}(x)
                    \leq\left(\sum_{t=1}^{T}
                    f_{t}\left(x^{(t)}\right)-f_{t}\left(x^{(t+1)}\right)\right)+R(x)-R\left(x^{(1)}\right)\)
                    <br>
                    i.e. \(R_T \leq \left(\sum_{t=1}^{T}
                    f_{t}\left(x^{(t)}\right)-f_{t}\left(x^{(t+1)}\right)\right)+\max _{x \in K}
                    R(x)-R\left(x^{(1)}\right) \)
                </li>
                <li class="note-item">
                    \(R\) should be spread out to fight the tendency of \(x\) to be very concentrated <br>
                    smaller if distribution is spread out, larger if distribution is concentrated
                </li>
            </ul>
        </li>
    
    
    </ul>
</div>

<!-- Footer -->
<iframe class="footer-frame" src="../../assets/frames/footer.html"></iframe>

<!-- Bootstrap core JavaScript -->
<script src="../../assets/vendor/jquery/jquery.min.js"></script>
<script src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Custom scripts for this template -->
<script src="../../assets/js/clean-blog.min.js"></script>

</body>
</html>
