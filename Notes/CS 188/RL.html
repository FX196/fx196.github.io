<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <title>CS 188 | RL</title>
    
    <!-- Bootstrap core CSS -->
    <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Custom fonts for this template -->
    <link href="../../assets/vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
          type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
          rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Quattrocento" rel="stylesheet">
    
    <!-- MathJax Import -->
    <script async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
            type="text/javascript">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    
    <script language="JavaScript" src="../../assets/js/main.js" type="text/javascript"></script>
    
    <!-- Custom styles for this template -->
    <link href="../../assets/css/main.css" rel="stylesheet">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137822561-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-137822561-1');
    </script>


</head>
<body>

<div class="linkshare-image">
    <img src="../../assets/img/meta-logo.jpeg" alt="unable to display image">
</div>
<!-- Header and Nav Bar -->
<iframe id="header-frame" onclick="console.log('a')" scrolling="no" src="../../assets/frames/header.html"></iframe>


<h2 align="center" class="notes-title" style="padding-bottom: 20px">Reinforcement Learning</h2>

<div class="container note-section">
    <ul>
        <li class="note-block">
            <h3>Basics</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Still Assume MDP
                </li>
                <li class="note-item">
                    \(T(s, a, s')\) and \(R(s, a, s')\) are unknown now
                </li>
            </ul>
        </li>
        <li class="note-block">
            <h3>Model-Based Learning</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Build an empirical model based on experiences
                </li>
                <li class="note-item">
                    \(\hat T(s, a, s')\) and \(\hat R(s, a, s')\) are estimated from experience
                    (by averaging seen values)
                </li>
                <li class="note-item">
                    Solve MDP with \(\hat T(s, a, s')\) and \(\hat R(s, a, s')\)
                </li>
            </ul>
        </li>
        <li class="note-block">
            <h3>Model-Free Learning</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Instead of using \(T\) and \(R\), take actions and the actual outcomes to compute the expected
                    outcome
                </li>
                <li class="note-item">
                    A method is model-free if it never uses \(T\) and \(R\)
                </li>
            </ul>
        </li>
        <li class="note-block">
            <h3>Passive Reinforcement Learning</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Take in a policy to follow and learns the value of states under that policy
                </li>
                <li class="note-item">
                    Does not update the policy
                </li>
                <li class="note-item">
                    <b>Direct Evaluation</b>: directly add up the rewards experienced, starting from that state, and
                    average w.r.t. episodes
                </li>
                <li class="note-item">
                    Direct evaluation discards information about state transitions, calculates states independently
                </li>
                <li class="note-item">
                    <b>Temporal Difference Learning</b>: <br>
                    \(sample = R(s, \pi(s), s') + \gamma V^{\pi}(s')\) <br>
                    \(V^{\pi}(s) = V^{\pi}(s) + \alpha (sample - V^{\pi}(s))\)
                </li>
                <li class="note-item">
                    \(\alpha\) is learning rate, or how important the new information is relative to old information;
                    <br>
                    e.g. if \(\alpha=0\) all old information is kept, if \(\alpha = 1\) old information is completely
                    overwritten by new information
                </li>
            </ul>
        </li>
        <li class="note-block">
            <h3>Active Reinforcement Learning</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Policy and values are learned
                </li>
                <li class="note-item">
                    Q-Learning: sample based Q-value iteration
                </li>
                <li class="note-item">
                    \(sample = R(s,a,s') + \gamma \max \limits_{a'} Q(s',a')\) <br>
                    \(Q(s,a) = (1-\alpha)Q(s,a) + \alpha(sample)\)
                </li>
                <li class="note-item">
                    \(T(s,a,s')\) is implicitly learned since the SAS pairs with higher transition probability is seen
                    more often and hence have more weight
                </li>
                <li class="note-item">
                    Q-learning converges if all state-action pairs are seen infinitely often
                </li>
                <li class="note-item">
                    \(\pi(s) = \text{argmax}\ Q(s,a)\)
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Approximate Q-Learning</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    Q-learning only remembers the values of q-states (not relations) and needs to store all the values
                </li>
                <li class="note-item">
                    idea: use features and represent states with a feature vector
                </li>
                <li class="note-item">
                    <b>feature-based representation</b> allows us to generalize learning experiences
                </li>
                <li class="note-item">
                    keep a trainable weight vector
                </li>
                <li class="note-item">
                    weight update: <br>
                    \(\text {difference}=\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}}
                    Q\left(s^{\prime}, a^{\prime}\right)\right]-Q(s, a)\) <br>
                    difference: difference between sample and predicted q-value <br>
                </li>
                <li class="note-item">
                    \(w_{i} \leftarrow w_{i}+\alpha \cdot \text {difference} \cdot f_{i}(s, a)\) <br>
                    update weights after each sample
                </li>
            </ul>
        </li>
        
        <li class="note-block">
            <h3>Exploration and Exploitation</h3>
            <hr class="note-line">
            <ul class="note-list">
                <li class="note-item">
                    \(\epsilon\)-greedy: choose random action with probability \(\epsilon\)
                </li>
                <li class="note-item">
                    exploration functions: add some value to states less seen, replace \(Q\left(s^{\prime},
                    a^{\prime}\right)\) with \(f\left(s^{\prime}, a^{\prime}\right)\) <br>
                    \(Q(s, a) \leftarrow(1-\alpha) Q(s, a)+\alpha \cdot\left[R\left(s, a, s^{\prime}\right)+\gamma \max
                    _{a^{\prime}} f\left(s^{\prime}, a^{\prime}\right)\right]\)
                </li>
                <li class="note-item">
                    usually, \(f(s, a)=Q(s, a)+\frac{k}{N(s, a)}\) where \(N(s, a)\) is the number of times \((s,a)\)
                    has been seen <br>
                    exploration term gets smaller as \(N(s,a)\) increases
                </li>
            </ul>
        </li>
    
    </ul>
</div>

<!-- Footer -->
<iframe class="footer-frame" src="../../assets/frames/footer.html"></iframe>

<!-- Bootstrap core JavaScript -->
<script src="../../assets/vendor/jquery/jquery.min.js"></script>
<script src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Custom scripts for this template -->
<script src="../../assets/js/clean-blog.min.js"></script>


</body>
</html>